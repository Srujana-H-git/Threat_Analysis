# ============================================
# Aerial CNN + Threat Scoring (Keras)
# ============================================
import os, json, random
import numpy as np
import matplotlib.pyplot as plt

# (Colab usually has these; keep as imports only)
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# -------------------------------
# Reproducibility & Config
# -------------------------------
SEED = 42
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

DATA_ROOT = "DOTA_subset"
TRAIN_DIR = os.path.join(DATA_ROOT, "train")
VAL_DIR   = os.path.join(DATA_ROOT, "val")   # folder name "val" (not "valid")
TEST_DIR  = os.path.join(DATA_ROOT, "test")

IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 10

# Threat weights (0â€“10 scale). Adjust to your policy.
# Final threat score = weight * confidence * 10  => 0â€“100 scale
THREAT_WEIGHTS = {
    "airport": 9.0,
    "ship": 7.5,
    "helipad": 6.0,
    "harbour": 8.0,
    "other": 2.0,     # fallback class (only if present)
}

# -------------------------------
# Sanity checks for folders
# -------------------------------
def _assert_dir(d, name):
    if not os.path.isdir(d):
        raise FileNotFoundError(f"{name} dir not found: {d}")

_assert_dir(TRAIN_DIR, "TRAIN")
_assert_dir(VAL_DIR,   "VAL")
if not os.path.isdir(TEST_DIR):
    print(f"[WARN] TEST dir not found: {TEST_DIR}. Will use VAL as TEST for evaluation & threat scoring.")
    TEST_DIR = VAL_DIR

# -------------------------------
# Data generators
# -------------------------------
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)
eval_datagen = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    seed=SEED
)
val_gen = eval_datagen.flow_from_directory(
    VAL_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    seed=SEED
)
test_gen = eval_datagen.flow_from_directory(
    TEST_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False  # important: keep order for reports
)

CLASS_NAMES = list(train_gen.class_indices.keys())
NUM_CLASSES = len(CLASS_NAMES)
print("Classes:", CLASS_NAMES)

# -------------------------------
# Model (simple CNN baseline)
# -------------------------------
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation="relu", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation="relu"),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(128, (3, 3), activation="relu"),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(NUM_CLASSES, activation="softmax")
])

model.compile(optimizer="adam",
              loss="categorical_crossentropy",
              metrics=["accuracy"])

model.summary()

# -------------------------------
# Training
# -------------------------------
history = model.fit(
    train_gen,
    epochs=EPOCHS,
    validation_data=val_gen
)

# -------------------------------
# Save model
# -------------------------------
os.makedirs("artifacts", exist_ok=True)
model_path = "artifacts/dota_cnn.h5"
model.save(model_path)
print(f"âœ… Model saved at {model_path}")

# -------------------------------
# Training curves
# -------------------------------
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history["accuracy"], label="train_acc")
plt.plot(history.history["val_accuracy"], label="val_acc")
plt.title("Accuracy"); plt.xlabel("Epochs"); plt.ylabel("Accuracy"); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.title("Loss"); plt.xlabel("Epochs"); plt.ylabel("Loss"); plt.legend()

curves_path = "artifacts/training_curves.png"
plt.savefig(curves_path, bbox_inches="tight")
print(f"ðŸ“Š Training curves saved as {curves_path}")
plt.show()

# -------------------------------
# Evaluation on TEST
# -------------------------------
print("\n--- Evaluation on TEST set ---")
test_loss, test_acc = model.evaluate(test_gen, verbose=0)
print(f"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}")

probs = model.predict(test_gen, verbose=0)          # (N, C)
y_pred = probs.argmax(axis=1)                       # predicted class indices
y_true = test_gen.classes                           # true class indices (from generator)
idx_to_class = {v:k for k,v in test_gen.class_indices.items()}

# Classification report (per-class precision/recall/F1)
report = classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4)
print("\nClassification Report:\n", report)

with open("artifacts/classification_report.txt", "w") as f:
    f.write(report)
print("ðŸ“ Saved artifacts/classification_report.txt")

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
plt.imshow(cm, cmap="Blues")
plt.title("Confusion Matrix")
plt.colorbar()
plt.xticks(np.arange(NUM_CLASSES), CLASS_NAMES, rotation=45, ha="right")
plt.yticks(np.arange(NUM_CLASSES), CLASS_NAMES)
plt.xlabel("Predicted"); plt.ylabel("True")
for i in range(NUM_CLASSES):
    for j in range(NUM_CLASSES):
        plt.text(j, i, cm[i, j], ha="center", va="center", color="black")
cm_path = "artifacts/confusion_matrix.png"
plt.tight_layout()
plt.savefig(cm_path, bbox_inches="tight")
print(f"ðŸ“Š Confusion matrix saved as {cm_path}")
plt.show()

# -------------------------------
# Threat Scoring (per-image)
# -------------------------------
# Threat score = (weight 0â€“10) * (prediction confidence 0â€“1) * 10 -> scale 0â€“100
# Unseen classes default to weight of 'other' or 2.0 if not provided.
weights = {c: THREAT_WEIGHTS.get(c, THREAT_WEIGHTS.get("other", 2.0)) for c in CLASS_NAMES}

# Reconstruct filepaths in the same order Keras used
filepaths = test_gen.filepaths  # aligned with y_true and probs

results = []
for path, pred_idx, prob_vec in zip(filepaths, y_pred, probs):
    pred_class = idx_to_class[pred_idx]
    conf = float(np.max(prob_vec))
    base_w = float(weights.get(pred_class, THREAT_WEIGHTS.get("other", 2.0)))
    score = base_w * conf * 10.0  # 0â€“100
    level = "LOW"
    if score >= 70: level = "HIGH"
    elif score >= 40: level = "MEDIUM"

    results.append({
        "image": path,
        "pred_class": pred_class,
        "confidence": round(conf, 4),
        "threat_weight": base_w,
        "threat_score": round(score, 2),
        "level": level
    })

# Save per-image results (CSV & JSON)
import csv
csv_path = "artifacts/threat_results.csv"
with open(csv_path, "w", newline="") as f:
    w = csv.DictWriter(f, fieldnames=list(results[0].keys()))
    w.writeheader(); w.writerows(results)
print(f"ðŸ§¾ Threat results (per image) saved to {csv_path}")

json_path = "artifacts/threat_results.json"
with open(json_path, "w") as f:
    json.dump({
        "class_names": CLASS_NAMES,
        "threat_weights": weights,
        "thresholds": {"LOW": "<40", "MEDIUM": "40-69.9", "HIGH": ">=70"},
        "results": results
    }, f, indent=2)
print(f"ðŸ§¾ Threat results JSON saved to {json_path}")

# Summary chart of threat levels
levels = [r["level"] for r in results]
summary = {k: levels.count(k) for k in ["LOW","MEDIUM","HIGH"]}
plt.figure(figsize=(5,3.5))
plt.bar(summary.keys(), summary.values())
plt.title("Threat Level Distribution (Test Set)")
plt.ylabel("Count")
summary_path = "artifacts/threat_summary.png"
plt.tight_layout()
plt.savefig(summary_path, bbox_inches="tight")
print(f"ðŸ“Š Threat summary saved as {summary_path}")
plt.show()

print("\nDone. Artifacts written to ./artifacts:")
print(" - dota_cnn.h5")
print(" - training_curves.png")
print(" - classification_report.txt")
print(" - confusion_matrix.png")
print(" - threat_results.csv")
print(" - threat_results.json")
print(" - threat_summary.png")
